{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSbM/2sap4g/sQPiW5ow8M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#09 - 1 순차 데이터와 순환 신경망"],"metadata":{"id":"tkRQlJSeDBa-"}},{"cell_type":"markdown","source":["## 순차 데이터\n","\n","순차 데이터(sequential data)는 텍스트나 시계열 데이터(time series data)와 같이 순서에 의미가 있는 데이터를 말한다.\n","\n","지금까지 우리가 보았던 데이터는 순서와  상관없었다.\n","\n","하지만 순차 데이터는 순서가 중요하다.\n","완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없다.\n","하나의 샘플을 사용하여 정방향 계산을 수행하고 나면 그 샘플은 다음 샘플을 처리할 때 재사용하지 않는다.\n","\n","\n","이렇게 입력 데이터의 흐름이 앞으로만 흐름이 전달되는 신경망을 피드포워드 신경망(feedforward neural network, FFNN)이라고 한다. 이전 장에서 배웠던 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망에 속한다,\n","\n","신경망이 전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용하기 위해서는 이렇게 데이터 흐름이 앞으로만 전달되면 곤란하다. 다음 샘플을 이전 데이터가 신경망 층에 순환될 필요가 있다. 이런 신경망이 바로 순환 신경망이다."],"metadata":{"id":"IYyHrjxhDKlc"}},{"cell_type":"markdown","source":["##순환 신경망\n","\n","순환 신경망(recurrent neural network, RNN)은 일반적인 완전 연결 신경망과 거의 비슷하다. 완전 연결 신경망 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 된다.\n","\n","뉴런의 출력이 다시 자기 자신에게 돌아온다. 즉, 어떤 샘플을 처리할 때 바로 이전에 사용했던 데이터를 재사용하는 셈이다.\n","\n","순환 신경망에서는 '이전 샘플에 대한 기억을 가지고 있다'고 종종 말한다. 이렇게 샘플을 처리하는 한 단계를 타임스텝(timestep)이라고 한다.\n","\n","순환 신경망은 이전 타임스템의 샘플을 기억하지만 타임 스템이 오래될수록 순환되는 정보는 희미해진다.\n","\n","순환 신경망에서는 특별히 층을 셀(cell)이라고 부른다. 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 은닉 상태(hidden state)라고 부른다.\n","\n","\n","합성곱 신경망에서처럼 신경망의 구조마다 조금씩 부르는 이름이 다를 수 있다. 하지만 기본 구조는 같다.\n","\n","모든 타임스텝에서 사용되는 가중치는 w_h하나라는 점이다. 가중치 w_h는 타임스텝에 따라 변화되는 뉴런의 출력을 학습한다. 이런 능력이 이 절의 시작 부분에 언급했던 순차 데이터를 다루는 데 필요한다.\n","\n","일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트 함수인 $tanh^2$가 많이 사용된다.\n","\n","다른 신경망과 마찬가지로 순환 신경망 그림에도 번거로움을 피하기 위해 활성화 함수를 표시하지 않는 경우가 많다. 하지만 순환 신경망에도 활성화 함수가 반드시 필요하다는 것을 꼭 기억하자.\n","\n","합성곱 신경망과 같은 피드포워드 신경망에서 뉴런은 입력과 가중치를 곱한다. 순환 신경망에서도 동일하다. 다만 순환 신경망의 뉴런은 가중치가 하나 더 있다. 바로 이전 타임스텝의 은닉 상태에 곱해지는 가중치이다. 셀은 입력과 이전 타임스텝의 은닉 상태를 사용하여 현재 타임스텝의 은닉 상태를 만든다."],"metadata":{"id":"ETtCCSpIEPVF"}},{"cell_type":"markdown","source":["## 셀의 가중치와 입출력\n","\n","순환 신경망의 셀에서  필요한 가중치 크기를 계산해보자. 복잡한 모델을 배울수록 가중치 개수를 계산해 보면 잘 이해하고 있는지 알 수 있다.\n","\n","예를 들어 순환층에 입력되는 특성의 개수가 4개이고 순환층의 뉴런이 3개라고 가정해보자.\n","\n","w_x의 크기를 구해보자. 입력층과 순황층의 뉴런이 모두 완전 연결되기 때문에 가중치w_x의 크기는 4 x 3 = 12개가 된다. 7장에서 본 완전 연결 신경망의 입력층과 은닉층의 연결과 같다.\n","\n","순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 w_h의 크기는 어떻게 될까?\n","\n","순환층에 있는 첫 번째 뉴런(r_1)의 은닉 상태가 다음 타임스텝에 재사용될 때 첫 번째 뉴런과 두 번째, 세 번째 뉴런에 모두 전달된다. 즉 이전 타임스텝의 은닉 상태는 다음 타입스텝의 뉴런에 완전히 연결된다.\n","\n","두 번째, 세 번째 뉴런의 은닉 상태도 동일하다.\n","\n","따라서 이 순환층에서 은닉 상태를 위한 가중치는 w_h는 3 x 3 = 9개이다.\n","\n","모델파라미터의 수는 w_x + w_h + 절편 = 12 + 9 + 3 = 24개 이다.\n","\n","순환층의 가중치 크기를 알아보았으므로 이번에는 순환층의 입력과 출력에 대해 생각해보자.\n","\n","합성곱 층의 입력은 전형적으로 하나의 샘플이 3개의 차원을 가진다.\n","순환층은 일반적으로 샘플마다 2개의 차원을 가진다. 보통 하나의 샘플을 하나의 시퀀스(sequence)라고 말한다. 시퀀스 안에는 여러개의 아이템이 들어있다. 여기에서 시퀀스의 길이가 바로 타임스텝의 길이가 된다.\n","\n","하나의 샘플은 시퀀스 길이와 단어 표현의 2차원 배열이다. 순환층을 통과하면 1차원 배열로 바뀐다. 이 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정된다.\n","\n","사실 순환층은 기본적으로 마지막 타임스텝의 은닉 상태로만 출력을 내보낸다.\n","\n","이는 마치 입력된 시퀀스 길이를 모두 읽어서 정보를 마지막 은닉 상태에 압축하여 전달하는 것처럼 볼 수 있다. 이제 순환 신경망이 정보를 기억하는 메모리를 가진다고 표현하는지 이해 할 수 있다. 또 순환 신경망이 순차 데이터에 잘 맞는 이유를 파악할 수 있다. 순환 신경망도 완전 연결 신경망이나 합성곱 신경망처럼 여러 개의 층을 쌓을 수 있다. 순환층을 여러 개 쌓았을 때는 셀의 출력은 어떻게 달라질까? 셀의 입력은 샘플마다 타임스텝과 단어 표현으로 이루어진 2차원 배열이어야 한다. 따라서 첫 번째 셀이 마지막 타임스텝의 은닉 상태만 출력해서는 안된다. 이런 경우에는 마지막 셀을 제외한 다른 모든 셀은 모든 타임스텝의 은닉상태를 출력한다.\n","\n","마지막으로 출력층의 구성에 대해 알아보자. 합성곱 신경망과 마찬가지로 순환 신경망도 마지막에는 밀집층을 두어 클래스를 분류한다. 다중 분류일 경우에는 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용한다. 이진 분류일 경우에는 하나의 뉴런을 두고 시그모이드 활성화 함수를 사용한다.\n","\n","합성곱 신경망과 다른 점은 마지막 셀의 출력이 1차원 이기 때문에 Flatten 클래스로 펼칠 필요가 없다. 셀의 출력을 그대로 밀집층에 사용할 수 있다."],"metadata":{"id":"l7a07-jTHRkc"}}]}